<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Theoretical Analysis of Multi-layer Neural Network: A Survey | Sharzy's random blog</title>
    <meta http-equiv="Content-Language"content="zh-cn"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1.0"/>
    <meta name="description" content="Sharzy's personal blog, used to record something worth recording"/>
    <meta name="keywords" content="mathwebCSMO"/>
    <meta name="author" content="Sharzy"/>
    <meta name="generator" content="jekyll"/>
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="/assets/css/common.css" rel="stylesheet">
    <script src="/assets/nav/responsive-nav.js"></script>
    <script src="/assets/js/page_util.js"></script>
    
     <link rel="stylesheet" href="/assets/css/highlight.css">  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>  <script type="text/javascript" src="/assets/nav/zooming.js"></script> 
    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
<div class="header">
    <div id="nav-wrap">
    <a href="/" class="fa fa-home fa-2x show-on-mobile home-icon"></a>
    <div id="nav">
        <ul>
            <li class="hidden-on-mobile"><a href="/" class="fa fa-home fa-2x"></a></li>
            <li><a class="bold" href="/">HOME</a></li>
            <li><a class="bold" href="/about">ABOUT</a></li>
            <li><a class="bold" href="/clock/clock.html">CLOCK</a></li>
        </ul>
    </div>
</div>

<script>
    let navigation = responsiveNav("#nav", {
        animate: true,
        transition: 400,
        label: '',
        init: function () {
            document.getElementsByClassName('nav-toggle')[0].classList.add('fa', 'fa-navicon');
        },
    });
    document.body.addEventListener('click', () => navigation.close())
</script>
    <div class="article-banner banner">
        <div class="article-tags">
            <i class="fa fa-tags"></i>
            
            <a class="tag header-tag" href="/?tags=AI,">AI,</a>
            
            <a class="tag header-tag" href="/?tags=论文">论文</a>
            
        </div>
        <p class="article-title bold">
            Theoretical Analysis of Multi-layer Neural Network: A Survey
        </p>
        <p class="article-subtitle">
            
        </p>
        <i class="article-date">
            Written on Nov 15th, 2019
        </i>
    </div>
</div>

<div class="main">
    <article class="main-text post-article">
        <p>本文是我在人工智能入门课上的期中论文。主要介绍关于多层神经网络的一些理论方面研究，包括可表达性，收敛性等。</p>

<h1 id="abstract">Abstract</h1>
<p>Though multi-layer neural network has been used in a wide range of areas, the explanation about how it works well is still insufficient. This survey primarily focus on the theoretical analysis about the approximation ability and training process of neural network. For approximation ability, researches show that it can be arbitrarily strong with the increase of network complexity. Qualified relationship between such ability and width/depth has been partially constructed. For training, we investigates the SGD algorithm. To analysis its performance, the trajectory around loss surface critical points are surveyed, and some theoretical results about SGD instances are summarized.</p>

<h1 id="table-of-content">Table of Content</h1>
<ul>
  <li>Introduction</li>
  <li>Approximation
    <ul>
      <li>Universal approximation theorem</li>
      <li>Width for approximation</li>
      <li>Depth for approximation</li>
    </ul>
  </li>
  <li>Training
    <ul>
      <li>The obstacle of SGD</li>
      <li>Instance analysis</li>
    </ul>
  </li>
  <li>Conclusion</li>
</ul>

<h1 id="document">Document</h1>
<p><a href="/assets/doc/survey_midterm.pdf">pdf file</a></p>

<h1 id="references">References</h1>

<p>[1]  Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning
via Over-Parameterization. arXiv:1811.03962 [cs, math, stat] , June 2019. 120.</p>

<p>[2]  A.R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930–945, May 1993. ISSN 0018-9448,
1557-9654. doi: 10.1109/18.256500. 2398.</p>

<p>[3]  Avrim Blum and Ronald L Rivest. Training a 3-Node Neural Network is NP-Complete.
page 8.</p>

<p>[4]  G Cybenkot. Approximation by superpositions of a sigmoidal function. page 12, 1989.</p>

<p>[5]  Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional
non-convex optimization. arXiv:1406.2572 [cs, math, stat] , June 2014.</p>

<p>[6]  Ronen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks.
page 34, 2016.</p>

<p>[7] Johan Ha, Benjamin Rossman, Rocco A Servedio, and Li-Yang Tan. An average-case depth
hierarchy theorem for Boolean circuits. Journal of the ACM, 9(4):29, 2016.</p>

<p>[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Im-
age Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, Las Vegas, NV, USA, June 2016. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.90.2.</p>

<p>[9] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward net-
works are universal approximators. Neural Networks, 2(5):359–366, January 1989. ISSN 08936080. doi: 10.1016/0893-6080(89)90020-8. 17000.8936081.</p>

<p>[10] Kenji Kawaguchi. Deep Learning without Poor Local Minima. page 9, 2016.
Yuanzhi Li and Yang Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU
Activation. page 11, 2017. 172.</p>

<p>[11] Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao.
Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A
review. Int. J. Autom. Comput., 14(5):503–519, October 2017. ISSN 1751-8520. doi:
10.1007/s11633-017-1054-2. part1.</p>

<p>[12] B. D. Ripley. Neural Networks and Related Methods for Classification. Journal of the Royal
Statistical Society. Series B (Methodological), 56(3):409–456, 1994.</p>

<p>[13] Maxwell Stinchcombe and Halbert White. Universal approximation using feedforward net-
works with non-sigmoid hidden layer activation functions — the Research Networking
System. 1989. 321.</p>

<p>[14] Matus Telgarsky. Benefits of depth in neural networks. page 23, 2016.</p>

<p>[15] X.-H. Yu. Can backpropagation error surface not have local minima. IEEE Trans. Neural
Netw., 3(6):1019–1021, Nov./1992. ISSN 10459227. doi: 10.1109/72.165604.</p>

<h1 id="ps">PS</h1>
<p>本文是我迄今为止写文章最肝的一次了。我选的这个 subarea 的论文非常杂，里面还有很多奇怪的 math，导致看进度论文缓慢。某些同学两天写完，我写了接近一个星期还只写了三分之二（关于 generalization 的问题本来想写的，但是时间所限，被我魔改删掉了）。可以预见的是，今后还会有很多更肝的事情即将发生。</p>

        <div id="post-switch">
            <div>
                
                <a href="/2019/11/10/triangle-short-art.html" id="post-previous">
                     <h5>
                         PREVIOUS ARTICLE
                     </h5>
                    多边形的三角形剖分结构及其应用
                </a>
                
            </div>

            <div>
                
                <a href="/2019/11/17/wechat-shit.html" id="post-next">
                    <h5>
                         NEXT POST
                    </h5>
                    为什么微信是垃圾
                </a>
                
            </div>
        </div>

        <div id="gitalk-container"></div>
    </article>

    <div class="sidebar hidden-on-mobile" id="sidebar">
        <div id="toc">
            <h5 class="toc-title">- Contents</h5>
        </div>
    </div>
</div>

<footer class="footer">
    <div class="container">
        <p style="margin: .5em">
            Designed by <a href="https://github.com/SharzyL">Sharzy</a>.
            Hosted on <a href="https://pages.github.com/">Github Pages</a>.
            Powered by <a href="https://jekyllrb.com/">Jekyll</a>.
        </p>
        <p>
            All text work on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
        </p>
    </div>
</footer>


<!-- use mathjax to handle formulas-->
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$']]}
        });
</script>

<!-- use sticky-sidebar to wrap table of contents to a sidebar-->
<script type="text/javascript">
    String.prototype.hashCode = function() {
        let hash = 0, i, chr;
        if (this.length === 0) return hash;
        for (i = 0; i < this.length; i++) {
            chr   = this.charCodeAt(i);
            hash  = ((hash << 5) - hash) + chr;
            hash |= 0; // Convert to 32bit integer
        }
        return Math.abs(hash);
    };
    const gitalk = new Gitalk({
        clientID: '56fd0766110ee8f26064',
        clientSecret: '81aca02e7bc69deccda1d2f5d258eed60b38a859',
        repo: 'SharzyL.github.io',
        owner: 'SharzyL',
        admin: ['SharzyL'],
        id: location.pathname.hashCode().toString(),      // Ensure uniqueness and length less than 50
        distractionFreeMode: true  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');

    document.addEventListener('DOMContentLoaded', () => {
        const zooming = new Zooming({

        });
        zooming.listen('article img');
    })
</script>

</body>
</html>