<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Distributed System for Deep learning Training: A Survey | Sharzy's random blog</title>
    <meta http-equiv="Content-Language" content="zh-cn"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1.0"/>
    <meta name="description" content="Sharzy's personal blog, used to record something worth recording"/>
    <meta name="keywords" content="mathwebCSMO"/>
    <meta name="author" content="Sharzy"/>
    <meta name="generator" content="jekyll"/>
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="/assets/css/common.css" rel="stylesheet">
    
     <link rel="stylesheet" href="/assets/css/highlight.css">  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" defer></script>  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" defer></script>  <script type="text/javascript" src="/assets/nav/zooming.js" defer></script> 
    <script src="/assets/nav/responsive-nav.js" defer></script>
    <script src="/assets/js/page_util.js" defer></script>
    <link href="https://netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
<div class="header">
    <div id="nav-wrap">
    <a href="/" class="fa fa-home fa-2x show-on-mobile home-icon"></a>
    <div id="nav">
        <ul style="padding-inline-start: 0">
            <li class="hidden-on-mobile"><a href="/" class="fa fa-home fa-2x"></a></li>
            <li class="hidden-on-mobile">
                <label for="search-line" id="search-btn">
                    <a class="fa fa-search"></a>
                </label>
                <input type="text" id="search-line" class="search-line-hidden">
            </li>
            <li>
                <a class="bold hover-underline" href="/">HOME</a>
            </li>
            <li><a class="bold hover-underline" href="/about">ABOUT</a></li>
            <li><a class="bold hover-underline" href="/clock/clock.html">CLOCK</a></li>
        </ul>
    </div>
</div>

    <div class="article-banner banner">
        <div class="article-tags">
            <i class="fa fa-tags"></i>
            
            <a class="tag header-tag" href="/?tags=AI">AI</a>
            
            <a class="tag header-tag" href="/?tags=paper">paper</a>
            
        </div>
        <p class="article-title bold">
            Distributed System for Deep learning Training: A Survey
        </p>
        <p class="article-subtitle">
            
        </p>
        <p class="article-date">
            <span class="inline-block">Posted on 2020-01-08</span>
            <span class="horizon-sep"></span>
            <span class="inline-block">Last edited on 2020-01-09</span>
        </p>
    </div>
</div>

<div class="main">
    <article class="main-text post-article">
        <p>某奇妙课程的期末论文。主要介绍分布式深度学习的架构和技术。</p>

<h1 id="abstract">Abstract</h1>

<p>Recent years have witnessed a growth in computation requirement to train modern deep neural networks with massive data volume and model size. Distributed systems are widely employed to accelerate the training process. In this article, we survey the principle and technology to construct such a system. Data parallelism and model parallelism are two fundamental strategies to parallelize the training process. Data parallelism separate training data to different nodes, while model parallelism partition the model. We summarize the architecture to utilize these strategies and how to minimize communication overhead and reach high scalability. Besides some compression techniques to accelerate data transmission are investigated in detail.</p>

<h1 id="table-of-content">Table of Content</h1>

<ul>
  <li>Introduction</li>
  <li>Data Parallelism
    <ul>
      <li>Parameer Server</li>
      <li>Synchronous SGD</li>
      <li>Asynchronous SGD</li>
      <li>Allreduce architecture</li>
      <li>Transmission Optimization</li>
    </ul>
  </li>
  <li>Model Parallelism
    <ul>
      <li>Compression Techniques</li>
      <li>Lower Arithmetic Precision</li>
      <li>Sparsification</li>
    </ul>
  </li>
</ul>

<h1 id="document">Document</h1>

<p><a href="/assets/doc/survey-final.pdf">pdf file</a></p>

<h1 id="references">References</h1>

<p>Alham Fikri Aji and Kenneth Heafield. Sparse Communication for Distributed Gradient
Descent. Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, pages 440–445, 2017. doi: 10.18653/v1/D17-1045.</p>

<p>Andrew Gibiansky. Bringing HPC Techniques to Deep Learning. http://research.
baidu.com/bringing-hpc-techniques-deep-learning/, 2017.</p>

<p>Karanbir Chahal, Manraj Singh Grover, and Kuntal Dey. A Hitchhiker’s Guide On Dis-
tributed Training of Deep Neural Networks. arXiv:1810.11787 [cs, stat], October 2018.</p>

<p>Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting
Distributed Synchronous SGD. arXiv:1604.00981 [cs], March 2017.</p>

<p>Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project
Adam: Building an Efficient and Scalable Deep Learning Training System. page 13, 2014.</p>

<p>Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural net-
works with low precision multiplications. arXiv:1412.7024 [cs], September 2015.</p>

<p>Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, and Andrew Y
Ng. Large Scale Distributed Deep Networks. page 9, 2012.</p>

<p>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, Miami, FL, June 2009. IEEE. ISBN 978-1-4244-
3992-8. doi: 10.1109/CVPR.2009.5206848.</p>

<p>Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep
Learning with Limited Numerical Precision. page 10, 2015.</p>

<p>Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang
Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen, Guangxiao Hu, Shaohuai
Shi, Xiaowen Chu, Tencent Inc, and Hong Kong Baptist University. Highly Scalable Deep
Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes.
page 9, 2018.</p>

<p>Peter H. Jin, Qiaochu Yuan, Forrest Iandola, and Kurt Keutzer. How to scale distributed
deep learning? arXiv:1611.04581 [cs], November 2016.</p>

<p>Jin Kyu Kim, Qirong Ho, Seunghak Lee, Xun Zheng, Wei Dai, Garth A. Gibson, and Eric P.
Xing. STRADS: A distributed framework for scheduled model parallel machine learning.
In Proceedings of the Eleventh European Conference on Computer Systems - EuroSys ’16,
pages 1–16, London, United Kingdom, 2016. ACM Press. ISBN 978-1-4503-4240-7. doi:
10.1145/2901318.2901331.</p>

<p>Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks.
arXiv:1404.5997 [cs], April 2014.</p>

<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep
convolutional neural networks. Commun. ACM, 60(6):84–90, 2012. ISSN 00010782. doi:
10.1145/3065386.</p>

<p>Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander Smola.
Parameter Server for Distributed Machine Learning. page 10, 2013.</p>

<p>Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep Gradient Com-
pression: Reducing the Communication Bandwidth for Distributed training. page 13,</p>

<p>Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou,
Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device Placement
Optimization with Reinforcement Learning. arXiv:1706.04972 [cs], June 2017.
Diego Ongaro and John Ousterhout. In Search of an Understandable Consensus Algorithm.
page 16, 2014.</p>

<p>Pitch Patarasuk and Xin Yuan. Bandwidth Efficient All-reduce Operation on Tree
Topologies. In 2007 IEEE International Parallel and Distributed Processing Sympo-
sium, pages 1–8, Long Beach, CA, USA, 2007. IEEE. ISBN 978-1-4244-0909-9. doi:
10.1109/IPDPS.2007.370405.</p>

<p>Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of
workstations. Journal of Parallel and Distributed Computing, 69(2):117–124, February 1. ISSN 07437315. doi: 10.1016/j.jpdc.2008.09.002.</p>

<p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. Int
J Comput Vis, 115(3):211–252, December 2015. ISSN 0920-5691, 1573-1405. doi:
10.1007/s11263-015-0816-y.</p>

<p>Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-Bit Stochastic Gradient De-
scent and its Application to Data-Parallel Distributed Training of Speech DNNs. page 5,</p>

<p>Alexander Sergeev and Mike Del Balso. Horovod: Fast and easy distributed deep learning
in TensorFlow. arXiv:1802.05799 [cs, stat], February 2018.</p>

<p>Nikko Ström. Sparse Connection and Pruning in Large Dynamic Artificial Neural Networks.
page 4, 1997.</p>

<p>Nikko Strom. Scalable Distributed DNN Training Using Commodity GPU Cloud Comput-
ing. page 5, 2015.</p>

<p>Rajeev Thakur, Rolf Rabenseifner, and William Gropp. Optimization of Collective Com-
munication Operations in MPICH. The International Journal of High Performance
Computing Applications, 19(1):49–66, February 2005. ISSN 1094-3420, 1741-2846. doi:
10.1177/1094342005051521.</p>

<p>Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R. Ganger,
Phillip B. Gibbons, Garth A. Gibson, and Eric P. Xing. Managed communication and
consistency for fast data-parallel iterative analytics. In Proceedings of the Sixth ACM
Symposium on Cloud Computing - SoCC ’15, pages 381–394, Kohala Coast, Hawaii, 1. ACM Press. ISBN 978-1-4503-3651-2. doi: 10.1145/2806777.2806778.</p>

<p>Hao Zhang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Gunhee Kim, Qirong Ho, and Eric Xing.
Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple
Machines. arXiv:1512.06216 [cs], December 2015a.</p>

<p>Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting Hu,
Jinliang Wei, Pengtao Xie, and Eric P Xing. Poseidon: An Effcient Communication
Architecture for Distributed Deep Learning on GPU Clusters. page 15, 2017.</p>

<p>Ruiliang Zhang and James T Kwok. Asynchronous Distributed ADMM for Consensus
Optimization. page 9, 2014.</p>

<p>Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with Elastic Averaging
SGD. page 9, 2015b.</p>

<p>Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and
Tie-Yan Liu. Asynchronous Stochastic Gradient Descent with Delay Compensation.
arXiv:1609.08326 [cs], August 2019.</p>

        <div id="post-switch">
            <div>
                
                <a href="/2019/12/19/structured-light.html" id="post-previous">
                     <h5>
                         PREVIOUS POST
                     </h5>
                    结构光技术——原理及其应用
                </a>
                
            </div>

            <div>
                
                <a href="/2020/01/09/latex-writing-flow.html" id="post-next">
                    <h5>
                         NEXT POST
                    </h5>
                    LaTeX 学术写作工作流
                </a>
                
            </div>
        </div>

        <div id="gitalk-container"></div>
    </article>

    <div class="sidebar hidden-on-mobile" id="sidebar">
        <div id="toc">
            <h5 class="toc-title">- Contents</h5>
        </div>
    </div>
</div>

<footer class="footer">
    <div class="container">
        <p style="margin: .5em">
            Designed by <a href="https://github.com/SharzyL">Sharzy</a>.
            Hosted on <a href="https://pages.github.com/">Github Pages</a>.
            Powered by <a href="https://jekyllrb.com/">Jekyll</a>.
        </p>
        <p>
            Content on this site is licensed under
            <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            unless specified.
        </p>
        <p>
            Subscribe this site via <a href="/feed.xml">RSS</a>
        </p>
    </div>
</footer>


<script>
    if ( window.ActiveXObject || "ActiveXObject" in window ) {
        window.location.href = '/ie-redirect.html';
    }
</script>
</body>
</html>